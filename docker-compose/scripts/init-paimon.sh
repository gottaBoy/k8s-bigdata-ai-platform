#!/bin/bash

# Paimonåˆå§‹åŒ–è„šæœ¬
# ä½œè€…: äº‘åŸç”Ÿä¸“å®¶
# ç‰ˆæœ¬: 1.0.0

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ç­‰å¾…Flinkå°±ç»ª
wait_for_flink() {
    log_info "ç­‰å¾…Flink JobManagerå°±ç»ª..."
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "http://flink-jobmanager:8081" > /dev/null 2>&1; then
            log_success "Flink JobManagerå·²å°±ç»ª"
            return 0
        fi
        
        log_info "å°è¯• $attempt/$max_attempts: Flinkæœªå°±ç»ªï¼Œç­‰å¾…10ç§’..."
        sleep 10
        attempt=$((attempt + 1))
    done
    
    log_error "Flinkå¯åŠ¨è¶…æ—¶"
    return 1
}

# ä¸‹è½½Paimon JARåŒ…
download_paimon_jars() {
    log_info "ä¸‹è½½Paimon JARåŒ…..."
    
    # åˆ›å»ºlibç›®å½•
    mkdir -p /opt/flink/lib
    
    # ä¸‹è½½Paimon JARåŒ…
    cd /opt/flink/lib
    
    # ä¸‹è½½Paimon Flink 1.17 JARåŒ…
    wget -q https://repo1.maven.org/maven2/org/apache/paimon/paimon-flink-1.17/0.6.0-incubating/paimon-flink-1.17-0.6.0-incubating.jar
    
    # ä¸‹è½½Paimon Hive Catalog JARåŒ…
    wget -q https://repo1.maven.org/maven2/org/apache/paimon/paimon-hive-catalog/0.6.0-incubating/paimon-hive-catalog-0.6.0-incubating.jar
    
    # ä¸‹è½½Paimon Filesystem JARåŒ…
    wget -q https://repo1.maven.org/maven2/org/apache/paimon/paimon-filesystem/0.6.0-incubating/paimon-filesystem-0.6.0-incubating.jar
    
    # ä¸‹è½½Paimon S3 JARåŒ…
    wget -q https://repo1.maven.org/maven2/org/apache/paimon/paimon-s3/0.6.0-incubating/paimon-s3-0.6.0-incubating.jar
    
    log_success "Paimon JARåŒ…ä¸‹è½½å®Œæˆ"
}

# åˆ›å»ºPaimon Catalog
create_paimon_catalog() {
    log_info "åˆ›å»ºPaimon Catalog..."
    
    # ç­‰å¾…Flink SQLå®¢æˆ·ç«¯å°±ç»ª
    sleep 10
    
    # åˆ›å»ºCatalog SQL
    cat > /tmp/create_catalog.sql << 'EOF'
-- åˆ›å»ºPaimon Catalog
CREATE CATALOG paimon WITH (
    'type' = 'paimon',
    'warehouse' = 's3://bigdata-lake/paimon',
    's3.endpoint' = 'http://minio:9000',
    's3.access-key' = 'admin',
    's3.secret-key' = 'minio123',
    's3.path.style' = 'true',
    's3.region' = 'us-east-1'
);

-- ä½¿ç”¨Paimon Catalog
USE CATALOG paimon;

-- åˆ›å»ºé»˜è®¤æ•°æ®åº“
CREATE DATABASE IF NOT EXISTS default_database;
USE default_database;
EOF

    # æ‰§è¡ŒSQL
    /opt/flink/bin/sql-client.sh -f /tmp/create_catalog.sql
    
    log_success "Paimon Catalogåˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºPaimonè¡¨
create_paimon_tables() {
    log_info "åˆ›å»ºPaimonè¡¨..."
    
    # ç”¨æˆ·äº‹ä»¶è¡¨
    cat > /tmp/create_user_events_table.sql << 'EOF'
-- åˆ›å»ºç”¨æˆ·äº‹ä»¶è¡¨
CREATE TABLE IF NOT EXISTS user_events (
    user_id BIGINT,
    event_type STRING,
    event_time TIMESTAMP(3),
    properties STRING,
    event_date AS DATE(event_time),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'bucket' = '4',
    'bucket-key' = 'user_id',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (event_date);
EOF

    # é”€å”®æ•°æ®è¡¨
    cat > /tmp/create_sales_table.sql << 'EOF'
-- åˆ›å»ºé”€å”®æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS sales_data (
    id BIGINT,
    product_name STRING,
    category STRING,
    price DECIMAL(10,2),
    quantity INT,
    sale_date DATE,
    region STRING,
    sale_timestamp AS CAST(sale_date AS TIMESTAMP(3)),
    WATERMARK FOR sale_timestamp AS sale_timestamp - INTERVAL '1' DAY
) WITH (
    'bucket' = '4',
    'bucket-key' = 'id',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (sale_date);
EOF

    # è¯é¢‘ç»Ÿè®¡è¡¨
    cat > /tmp/create_word_count_table.sql << 'EOF'
-- åˆ›å»ºè¯é¢‘ç»Ÿè®¡è¡¨
CREATE TABLE IF NOT EXISTS word_count (
    word STRING,
    count BIGINT,
    window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
    window_date AS DATE(window_start),
    WATERMARK FOR window_start AS window_start - INTERVAL '5' SECOND
) WITH (
    'bucket' = '4',
    'bucket-key' = 'word',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (window_date);
EOF

    # æ‰§è¡ŒSQL
    /opt/flink/bin/sql-client.sh -f /tmp/create_user_events_table.sql
    /opt/flink/bin/sql-client.sh -f /tmp/create_sales_table.sql
    /opt/flink/bin/sql-client.sh -f /tmp/create_word_count_table.sql
    
    log_success "Paimonè¡¨åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºç¤ºä¾‹æ•°æ®
create_sample_data() {
    log_info "åˆ›å»ºç¤ºä¾‹æ•°æ®..."
    
    # æ’å…¥ç”¨æˆ·äº‹ä»¶æ•°æ®
    cat > /tmp/insert_user_events.sql << 'EOF'
-- æ’å…¥ç”¨æˆ·äº‹ä»¶ç¤ºä¾‹æ•°æ®
INSERT INTO user_events VALUES
(1, 'click', TIMESTAMP '2024-01-01 10:00:00', '{"page": "/home", "source": "web"}'),
(2, 'purchase', TIMESTAMP '2024-01-01 10:05:00', '{"product_id": "123", "amount": 99.99}'),
(3, 'view', TIMESTAMP '2024-01-01 10:10:00', '{"page": "/product", "duration": 30}'),
(4, 'click', TIMESTAMP '2024-01-01 10:15:00', '{"page": "/cart", "source": "mobile"}'),
(5, 'logout', TIMESTAMP '2024-01-01 10:20:00', '{"session_duration": 1200}');
EOF

    # æ’å…¥é”€å”®æ•°æ®
    cat > /tmp/insert_sales_data.sql << 'EOF'
-- æ’å…¥é”€å”®æ•°æ®ç¤ºä¾‹
INSERT INTO sales_data VALUES
(1, 'Laptop', 'Electronics', 999.99, 2, DATE '2024-01-01', 'North'),
(2, 'Phone', 'Electronics', 599.99, 5, DATE '2024-01-02', 'South'),
(3, 'Book', 'Education', 29.99, 10, DATE '2024-01-03', 'East'),
(4, 'Chair', 'Furniture', 199.99, 3, DATE '2024-01-04', 'West'),
(5, 'Table', 'Furniture', 299.99, 1, DATE '2024-01-05', 'North');
EOF

    # æ‰§è¡Œæ’å…¥
    /opt/flink/bin/sql-client.sh -f /tmp/insert_user_events.sql
    /opt/flink/bin/sql-client.sh -f /tmp/insert_sales_data.sql
    
    log_success "ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºFlinkä½œä¸š
create_flink_jobs() {
    log_info "åˆ›å»ºFlinkä½œä¸š..."
    
    # è¯é¢‘ç»Ÿè®¡ä½œä¸š
    cat > /tmp/word_count_job.sql << 'EOF'
-- è¯é¢‘ç»Ÿè®¡æµå¤„ç†ä½œä¸š
CREATE TABLE word_count_input (
    word STRING,
    count BIGINT,
    ts TIMESTAMP(3),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'word-count-input',
    'properties.bootstrap.servers' = 'kafka:29092',
    'properties.group.id' = 'word-count-group',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json'
);

-- åˆ›å»ºè¯é¢‘ç»Ÿè®¡ä½œä¸š
INSERT INTO word_count
SELECT 
    word,
    SUM(count) as count,
    TUMBLE_START(ts, INTERVAL '1' MINUTE) as window_start,
    TUMBLE_END(ts, INTERVAL '1' MINUTE) as window_end
FROM word_count_input
GROUP BY word, TUMBLE(ts, INTERVAL '1' MINUTE);
EOF

    # ç”¨æˆ·äº‹ä»¶èšåˆä½œä¸š
    cat > /tmp/user_events_job.sql << 'EOF'
-- ç”¨æˆ·äº‹ä»¶èšåˆä½œä¸š
CREATE TABLE user_events_kafka (
    user_id BIGINT,
    event_type STRING,
    event_time TIMESTAMP(3),
    properties STRING,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'user-events',
    'properties.bootstrap.servers' = 'kafka:29092',
    'properties.group.id' = 'user-events-group',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json'
);

-- ç”¨æˆ·äº‹ä»¶èšåˆ
INSERT INTO user_events
SELECT 
    user_id,
    event_type,
    event_time,
    properties
FROM user_events_kafka;
EOF

    # æäº¤ä½œä¸š
    /opt/flink/bin/sql-client.sh -f /tmp/word_count_job.sql
    /opt/flink/bin/sql-client.sh -f /tmp/user_events_job.sql
    
    log_success "Flinkä½œä¸šåˆ›å»ºå®Œæˆ"
}

# éªŒè¯PaimonåŠŸèƒ½
verify_paimon() {
    log_info "éªŒè¯PaimonåŠŸèƒ½..."
    
    # æŸ¥è¯¢è¡¨åˆ—è¡¨
    cat > /tmp/verify_tables.sql << 'EOF'
-- æŸ¥çœ‹æ‰€æœ‰è¡¨
SHOW TABLES;

-- æŸ¥è¯¢ç”¨æˆ·äº‹ä»¶æ•°æ®
SELECT COUNT(*) as user_events_count FROM user_events;

-- æŸ¥è¯¢é”€å”®æ•°æ®
SELECT COUNT(*) as sales_count FROM sales_data;

-- æŸ¥è¯¢è¯é¢‘ç»Ÿè®¡
SELECT COUNT(*) as word_count_records FROM word_count;
EOF

    /opt/flink/bin/sql-client.sh -f /tmp/verify_tables.sql
    
    log_success "PaimonåŠŸèƒ½éªŒè¯å®Œæˆ"
}

# æ˜¾ç¤ºPaimonä¿¡æ¯
show_paimon_info() {
    log_info "=== Paimonæ•°æ®æ¹–ä¿¡æ¯ ==="
    echo ""
    echo "ğŸ“Š Paimon Catalog: paimon"
    echo "ğŸ  æ•°æ®ä»“åº“: s3://bigdata-lake/paimon"
    echo "ğŸ”— S3ç«¯ç‚¹: http://minio:9000"
    echo ""
    echo "ğŸ“‹ å·²åˆ›å»ºçš„è¡¨:"
    echo "  â€¢ user_events - ç”¨æˆ·äº‹ä»¶è¡¨"
    echo "  â€¢ sales_data - é”€å”®æ•°æ®è¡¨"
    echo "  â€¢ word_count - è¯é¢‘ç»Ÿè®¡è¡¨"
    echo ""
    echo "ğŸš€ Flinkä½œä¸š:"
    echo "  â€¢ è¯é¢‘ç»Ÿè®¡æµå¤„ç†ä½œä¸š"
    echo "  â€¢ ç”¨æˆ·äº‹ä»¶èšåˆä½œä¸š"
    echo ""
    echo "ğŸ”§ è®¿é—®æ–¹å¼:"
    echo "  â€¢ Flink SQL Client: /opt/flink/bin/sql-client.sh"
    echo "  â€¢ Flink Web UI: http://localhost:8081"
    echo "  â€¢ MinIO Console: http://localhost:9001"
    echo ""
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹åˆå§‹åŒ–Paimonæ•°æ®æ¹–..."
    
    # ç­‰å¾…Flinkå°±ç»ª
    wait_for_flink
    
    # ä¸‹è½½Paimon JARåŒ…
    download_paimon_jars
    
    # åˆ›å»ºPaimon Catalog
    create_paimon_catalog
    
    # åˆ›å»ºPaimonè¡¨
    create_paimon_tables
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    create_sample_data
    
    # åˆ›å»ºFlinkä½œä¸š
    create_flink_jobs
    
    # éªŒè¯PaimonåŠŸèƒ½
    verify_paimon
    
    # æ˜¾ç¤ºä¿¡æ¯
    show_paimon_info
    
    log_success "Paimonæ•°æ®æ¹–åˆå§‹åŒ–å®Œæˆï¼"
    log_info "Paimonå·²ä¸Flinkæ·±åº¦é›†æˆï¼Œæ”¯æŒæµå¼æ•°æ®æ¹–åŠŸèƒ½"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@" 