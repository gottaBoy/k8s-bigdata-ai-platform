#!/bin/bash

# æ•°æ®åˆå§‹åŒ–è„šæœ¬
# ä½œè€…: äº‘åŽŸç”Ÿä¸“å®¶
# ç‰ˆæœ¬: 1.0.0

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ç­‰å¾…æœåŠ¡å°±ç»ª
wait_for_service() {
    local service=$1
    local port=$2
    local max_attempts=30
    local attempt=1
    
    log_info "ç­‰å¾…æœåŠ¡ $service å°±ç»ª..."
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "http://localhost:$port" > /dev/null 2>&1; then
            log_success "$service å·²å°±ç»ª"
            return 0
        fi
        
        log_info "å°è¯• $attempt/$max_attempts: $service æœªå°±ç»ªï¼Œç­‰å¾…5ç§’..."
        sleep 5
        attempt=$((attempt + 1))
    done
    
    log_error "$service å¯åŠ¨è¶…æ—¶"
    return 1
}

# åˆå§‹åŒ–MinIO
init_minio() {
    log_info "åˆå§‹åŒ–MinIO..."
    
    # ç­‰å¾…MinIOå°±ç»ª
    wait_for_service "MinIO" 9000
    
    # åˆ›å»ºbucket
    docker exec minio-client mc mb myminio/bigdata-lake 2>/dev/null || true
    docker exec minio-client mc mb myminio/ml-models 2>/dev/null || true
    docker exec minio-client mc mb myminio/checkpoints 2>/dev/null || true
    docker exec minio-client mc mb myminio/savepoints 2>/dev/null || true
    
    # è®¾ç½®bucketç­–ç•¥
    docker exec minio-client mc policy set public myminio/bigdata-lake 2>/dev/null || true
    
    log_success "MinIOåˆå§‹åŒ–å®Œæˆ"
}

# åˆå§‹åŒ–Kafka
init_kafka() {
    log_info "åˆå§‹åŒ–Kafka..."
    
    # ç­‰å¾…Kafkaå°±ç»ª
    wait_for_service "Kafka" 9092
    
    # åˆ›å»ºä¸»é¢˜
    docker exec kafka kafka-topics --create --bootstrap-server localhost:9092 --topic user-events --partitions 3 --replication-factor 1 2>/dev/null || true
    docker exec kafka kafka-topics --create --bootstrap-server localhost:9092 --topic data-stream --partitions 3 --replication-factor 1 2>/dev/null || true
    docker exec kafka kafka-topics --create --bootstrap-server localhost:9092 --topic ml-events --partitions 3 --replication-factor 1 2>/dev/null || true
    docker exec kafka kafka-topics --create --bootstrap-server localhost:9092 --topic word-count-input --partitions 3 --replication-factor 1 2>/dev/null || true
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    log_info "ç”ŸæˆKafkaæµ‹è¯•æ•°æ®..."
    docker exec kafka bash -c '
        for i in {1..100}; do
            echo "{\"user_id\": $i, \"event_type\": \"click\", \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%S.000Z)\", \"properties\": {\"page\": \"/home\", \"source\": \"web\"}}" | kafka-console-producer --bootstrap-server localhost:9092 --topic user-events
            echo "{\"word\": \"hello\", \"count\": 1, \"ts\": \"$(date -u +%Y-%m-%dT%H:%M:%S.000Z)\"}" | kafka-console-producer --bootstrap-server localhost:9092 --topic word-count-input
            echo "{\"word\": \"world\", \"count\": 1, \"ts\": \"$(date -u +%Y-%m-%dT%H:%M:%S.000Z)\"}" | kafka-console-producer --bootstrap-server localhost:9092 --topic word-count-input
            sleep 1
        done
    ' &
    
    log_success "Kafkaåˆå§‹åŒ–å®Œæˆ"
}

# åˆå§‹åŒ–Doris
init_doris() {
    log_info "åˆå§‹åŒ–Doris..."
    
    # ç­‰å¾…Doris FEå°±ç»ª
    wait_for_service "Doris FE" 8030
    
    # ç­‰å¾…Doris BEå°±ç»ª
    wait_for_service "Doris BE" 8040
    
    # åˆ›å»ºæ•°æ®åº“å’Œè¡¨
    docker exec doris-fe mysql -h localhost -P 9030 -u root -e "
        CREATE DATABASE IF NOT EXISTS bigdata_platform;
        USE bigdata_platform;
        
        CREATE TABLE IF NOT EXISTS user_events (
            user_id BIGINT,
            event_type VARCHAR(50),
            event_time DATETIME,
            properties JSON
        ) DUPLICATE KEY(user_id, event_time)
        DISTRIBUTED BY HASH(user_id) BUCKETS 10
        PROPERTIES (
            'replication_num' = '1'
        );
        
        CREATE TABLE IF NOT EXISTS word_count (
            word VARCHAR(100),
            count BIGINT,
            window_start DATETIME,
            window_end DATETIME
        ) DUPLICATE KEY(word, window_start)
        DISTRIBUTED BY HASH(word) BUCKETS 10
        PROPERTIES (
            'replication_num' = '1'
        );
        
        CREATE TABLE IF NOT EXISTS sales_data (
            id BIGINT,
            product_name VARCHAR(100),
            category VARCHAR(50),
            price DECIMAL(10,2),
            quantity INT,
            sale_date DATE,
            region VARCHAR(50)
        ) DUPLICATE KEY(id, sale_date)
        DISTRIBUTED BY HASH(id) BUCKETS 10
        PROPERTIES (
            'replication_num' = '1'
        );
    " 2>/dev/null || true
    
    # æ’å…¥æµ‹è¯•æ•°æ®
    log_info "æ’å…¥Dorisæµ‹è¯•æ•°æ®..."
    docker exec doris-fe mysql -h localhost -P 9030 -u root -e "
        USE bigdata_platform;
        
        INSERT INTO sales_data VALUES
        (1, 'Laptop', 'Electronics', 999.99, 2, '2024-01-01', 'North'),
        (2, 'Phone', 'Electronics', 599.99, 5, '2024-01-02', 'South'),
        (3, 'Book', 'Education', 29.99, 10, '2024-01-03', 'East'),
        (4, 'Chair', 'Furniture', 199.99, 3, '2024-01-04', 'West'),
        (5, 'Table', 'Furniture', 299.99, 1, '2024-01-05', 'North');
    " 2>/dev/null || true
    
    log_success "Dorisåˆå§‹åŒ–å®Œæˆ"
}

# åˆå§‹åŒ–Redis
init_redis() {
    log_info "åˆå§‹åŒ–Redis..."
    
    # ç­‰å¾…Rediså°±ç»ª
    wait_for_service "Redis" 6379
    
    # æ’å…¥æµ‹è¯•æ•°æ®
    docker exec redis redis-cli -a redis123 SET "app:config:version" "1.0.0" 2>/dev/null || true
    docker exec redis redis-cli -a redis123 SET "app:stats:users:total" "1000" 2>/dev/null || true
    docker exec redis redis-cli -a redis123 SET "app:stats:orders:total" "5000" 2>/dev/null || true
    
    # åˆ›å»ºåˆ—è¡¨
    docker exec redis redis-cli -a redis123 LPUSH "recent_events" "user_login" "user_logout" "order_created" 2>/dev/null || true
    
    # åˆ›å»ºé›†åˆ
    docker exec redis redis-cli -a redis123 SADD "active_users" "user1" "user2" "user3" 2>/dev/null || true
    
    log_success "Redisåˆå§‹åŒ–å®Œæˆ"
}

# åˆå§‹åŒ–Airflow
init_airflow() {
    log_info "åˆå§‹åŒ–Airflow..."
    
    # ç­‰å¾…Airflowå°±ç»ª
    wait_for_service "Airflow" 8080
    
    # åˆ›å»ºç¤ºä¾‹DAG
    cat > /tmp/sample_dag.py << 'EOF'
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta
import requests
import json

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'data_pipeline_demo',
    default_args=default_args,
    description='æ•°æ®ç®¡é“æ¼”ç¤º',
    schedule_interval=timedelta(hours=1),
    catchup=False,
)

def extract_data():
    """æå–æ•°æ®"""
    print("å¼€å§‹æå–æ•°æ®...")
    # æ¨¡æ‹Ÿä»ŽKafkaæå–æ•°æ®
    return {"message": "æ•°æ®æå–å®Œæˆ", "count": 100}

def transform_data(**context):
    """è½¬æ¢æ•°æ®"""
    print("å¼€å§‹è½¬æ¢æ•°æ®...")
    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
    return {"message": "æ•°æ®è½¬æ¢å®Œæˆ", "processed_count": 95}

def load_data(**context):
    """åŠ è½½æ•°æ®"""
    print("å¼€å§‹åŠ è½½æ•°æ®...")
    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½åˆ°Doris
    return {"message": "æ•°æ®åŠ è½½å®Œæˆ", "loaded_count": 90}

def check_health():
    """å¥åº·æ£€æŸ¥"""
    print("æ‰§è¡Œå¥åº·æ£€æŸ¥...")
    services = ['minio:9000', 'kafka:9092', 'doris-fe:8030', 'redis:6379']
    for service in services:
        try:
            response = requests.get(f"http://{service}", timeout=5)
            print(f"{service} å¥åº·æ£€æŸ¥é€šè¿‡")
        except:
            print(f"{service} å¥åº·æ£€æŸ¥å¤±è´¥")

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)

health_check_task = PythonOperator(
    task_id='health_check',
    python_callable=check_health,
    dag=dag,
)

# ä»»åŠ¡ä¾èµ–
extract_task >> transform_task >> load_task
health_check_task >> extract_task
EOF

    # å¤åˆ¶DAGæ–‡ä»¶
    docker cp /tmp/sample_dag.py airflow-webserver:/opt/airflow/dags/
    
    log_success "Airflowåˆå§‹åŒ–å®Œæˆ"
}

# åˆå§‹åŒ–Superset
init_superset() {
    log_info "åˆå§‹åŒ–Superset..."
    
    # ç­‰å¾…Supersetå°±ç»ª
    wait_for_service "Superset" 8088
    
    log_info "Supersetåˆå§‹åŒ–å®Œæˆï¼Œè¯·æ‰‹åŠ¨é…ç½®æ•°æ®æº"
    log_info "è®¿é—®åœ°å€: http://localhost:8088"
    log_info "é»˜è®¤è´¦å·: admin/admin123"
}

# åˆå§‹åŒ–JupyterHub
init_jupyterhub() {
    log_info "åˆå§‹åŒ–JupyterHub..."
    
    # ç­‰å¾…JupyterHubå°±ç»ª
    wait_for_service "JupyterHub" 8000
    
    log_info "JupyterHubåˆå§‹åŒ–å®Œæˆ"
    log_info "è®¿é—®åœ°å€: http://localhost:8000"
    log_info "é»˜è®¤è´¦å·: admin/jupyter123"
}

# åˆå§‹åŒ–Paimon
init_paimon() {
    log_info "åˆå§‹åŒ–Paimonæ•°æ®æ¹–..."
    
    # ç­‰å¾…Flinkå°±ç»ª
    wait_for_service "Flink JobManager" 8081
    
    # ç­‰å¾…Paimon CatalogæœåŠ¡å°±ç»ª
    sleep 30
    
    # æ£€æŸ¥Paimon Catalogæ˜¯å¦å·²åˆå§‹åŒ–
    if docker exec paimon-catalog ls /opt/flink/lib/paimon-flink-1.17-*.jar > /dev/null 2>&1; then
        log_success "Paimonå·²åˆå§‹åŒ–"
    else
        log_info "Paimonåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™..."
        sleep 60
    fi
    
    log_info "Paimonæ•°æ®æ¹–åˆå§‹åŒ–å®Œæˆ"
    log_info "è®¿é—®Flink Web UI: http://localhost:8081"
    log_info "Paimon Catalog: paimon"
    log_info "æ•°æ®ä»“åº“: s3://bigdata-lake/paimon"
}

# åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶
create_sample_data() {
    log_info "åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶..."
    
    # åˆ›å»ºCSVç¤ºä¾‹æ•°æ®
    cat > /tmp/sample_sales.csv << 'EOF'
id,product_name,category,price,quantity,sale_date,region
1,Laptop,Electronics,999.99,2,2024-01-01,North
2,Phone,Electronics,599.99,5,2024-01-02,South
3,Book,Education,29.99,10,2024-01-03,East
4,Chair,Furniture,199.99,3,2024-01-04,West
5,Table,Furniture,299.99,1,2024-01-05,North
6,Monitor,Electronics,299.99,4,2024-01-06,South
7,Desk,Furniture,399.99,2,2024-01-07,East
8,Keyboard,Electronics,79.99,8,2024-01-08,West
9,Mouse,Electronics,49.99,12,2024-01-09,North
10,Notebook,Education,9.99,20,2024-01-10,South
EOF

    # ä¸Šä¼ åˆ°MinIO
    docker cp /tmp/sample_sales.csv minio-client:/tmp/
    docker exec minio-client mc cp /tmp/sample_sales.csv myminio/bigdata-lake/
    
    log_success "ç¤ºä¾‹æ•°æ®æ–‡ä»¶åˆ›å»ºå®Œæˆ"
}

# æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
show_access_info() {
    log_info "=== å¹³å°è®¿é—®ä¿¡æ¯ ==="
    echo ""
    echo "ðŸŒ æœåŠ¡è®¿é—®åœ°å€:"
    echo "  MinIO Console:     http://localhost:9001 (admin/minio123)"
    echo "  Kafka UI:          http://localhost:8082"
    echo "  Doris FE:          http://localhost:8030"
    echo "  Doris BE:          http://localhost:8040"
    echo "  Flink JobManager:  http://localhost:8081"
    echo "  Paimon Data Lake:  s3://bigdata-lake/paimon"
    echo "  Prometheus:        http://localhost:9090"
    echo "  Grafana:           http://localhost:3000 (admin/admin123)"
    echo "  Airflow:           http://localhost:8080 (admin/admin123)"
    echo "  Superset:          http://localhost:8088 (admin/admin123)"
    echo "  JupyterHub:        http://localhost:8000 (admin/jupyter123)"
    echo ""
    echo "ðŸ”§ è¿žæŽ¥ä¿¡æ¯:"
    echo "  MinIO:             localhost:9000 (admin/minio123)"
    echo "  Kafka:             localhost:9092"
    echo "  Doris:             localhost:9030 (root/root)"
    echo "  Redis:             localhost:6379 (redis123)"
    echo "  PostgreSQL:        localhost:5432 (airflow/airflow123)"
    echo ""
    echo "ðŸ“Š ç›‘æŽ§é¢æ¿:"
    echo "  Grafana:           http://localhost:3000"
    echo "  Prometheus:        http://localhost:9090"
    echo "  Kafka UI:          http://localhost:8082"
    echo ""
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹åˆå§‹åŒ–å¤§æ•°æ®AIå¹³å°æ•°æ®..."
    
    # ç­‰å¾…æ‰€æœ‰æœåŠ¡å¯åŠ¨
    sleep 30
    
    # åˆå§‹åŒ–å„ä¸ªæœåŠ¡
    init_minio
    init_kafka
    init_doris
    init_redis
    init_airflow
    init_superset
    init_jupyterhub
    init_paimon
    create_sample_data
    
    # æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
    show_access_info
    
    log_success "æ•°æ®åˆå§‹åŒ–å®Œæˆï¼"
    log_info "æ‰€æœ‰æœåŠ¡å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨å¹³å°"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@" 