# Paimonæ•°æ®æ¹–å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

Paimonæ˜¯ä¸€ä¸ªæµå¼æ•°æ®æ¹–å­˜å‚¨ç³»ç»Ÿï¼Œä¸Apache Flinkæ·±åº¦é›†æˆï¼Œæä¾›é«˜æ€§èƒ½çš„æµå¼è¯»å†™ã€ACIDäº‹åŠ¡ã€æ—¶é—´æ—…è¡ŒæŸ¥è¯¢ç­‰åŠŸèƒ½ã€‚æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹Paimonæ•°æ®æ¹–ã€‚

## ğŸ—ï¸ æ¶æ„ç‰¹ç‚¹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Paimonæ•°æ®æ¹–æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åº”ç”¨å±‚: Flink SQL + æµå¤„ç†ä½œä¸š                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å­˜å‚¨å±‚: Paimon Catalog + S3/MinIO                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ–‡ä»¶æ ¼å¼: Parquet + å¢é‡æ–‡ä»¶                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å…ƒæ•°æ®: å¿«ç…§ + æ¸…å•æ–‡ä»¶                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿Docker Composeç¯å¢ƒå·²å¯åŠ¨ï¼š

```bash
cd docker-compose
./start.sh
```

### 2. è®¿é—®Flink SQL Client

```bash
# è¿›å…¥Flink JobManagerå®¹å™¨
docker exec -it flink-jobmanager bash

# å¯åŠ¨SQL Client
/opt/flink/bin/sql-client.sh
```

### 3. åŸºç¡€æ“ä½œ

#### 3.1 ä½¿ç”¨Paimon Catalog

```sql
-- ä½¿ç”¨Paimon Catalog
USE CATALOG paimon;

-- æŸ¥çœ‹æ•°æ®åº“
SHOW DATABASES;

-- ä½¿ç”¨é»˜è®¤æ•°æ®åº“
USE default_database;

-- æŸ¥çœ‹è¡¨
SHOW TABLES;
```

#### 3.2 åˆ›å»ºè¡¨

```sql
-- åˆ›å»ºç”¨æˆ·äº‹ä»¶è¡¨
CREATE TABLE user_events (
    user_id BIGINT,
    event_type STRING,
    event_time TIMESTAMP(3),
    properties STRING,
    event_date AS DATE(event_time),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'bucket' = '4',
    'bucket-key' = 'user_id',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (event_date);
```

#### 3.3 æ’å…¥æ•°æ®

```sql
-- æ’å…¥ç¤ºä¾‹æ•°æ®
INSERT INTO user_events VALUES
(1, 'click', TIMESTAMP '2024-01-01 10:00:00', '{"page": "/home"}'),
(2, 'purchase', TIMESTAMP '2024-01-01 10:05:00', '{"product_id": "123"}'),
(3, 'view', TIMESTAMP '2024-01-01 10:10:00', '{"page": "/product"}');
```

#### 3.4 æŸ¥è¯¢æ•°æ®

```sql
-- æŸ¥è¯¢ç”¨æˆ·äº‹ä»¶
SELECT * FROM user_events WHERE event_date = '2024-01-01';

-- ç»Ÿè®¡äº‹ä»¶ç±»å‹
SELECT event_type, COUNT(*) as count 
FROM user_events 
GROUP BY event_type;
```

## ğŸ“Š æµå¼æ•°æ®å¤„ç†

### 1. åˆ›å»ºKafkaæ•°æ®æº

```sql
-- åˆ›å»ºKafkaç”¨æˆ·äº‹ä»¶æºè¡¨
CREATE TABLE kafka_user_events (
    user_id BIGINT,
    event_type STRING,
    event_time TIMESTAMP(3),
    properties STRING,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'user-events',
    'properties.bootstrap.servers' = 'kafka:29092',
    'properties.group.id' = 'paimon-group',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'json'
);
```

### 2. æµå¼å†™å…¥Paimon

```sql
-- ä»Kafkaæµå¼å†™å…¥Paimon
INSERT INTO user_events
SELECT user_id, event_type, event_time, properties
FROM kafka_user_events;
```

### 3. å®æ—¶èšåˆ

```sql
-- åˆ›å»ºå°æ—¶çº§èšåˆè¡¨
CREATE TABLE user_events_hourly (
    event_type STRING,
    event_hour TIMESTAMP(3),
    user_count BIGINT,
    event_count BIGINT,
    PRIMARY KEY (event_type, event_hour) NOT ENFORCED
) WITH (
    'bucket' = '4',
    'bucket-key' = 'event_type',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (event_hour);

-- å°æ—¶çº§èšåˆ
INSERT INTO user_events_hourly
SELECT 
    event_type,
    TUMBLE_START(event_time, INTERVAL '1' HOUR) as event_hour,
    COUNT(DISTINCT user_id) as user_count,
    COUNT(*) as event_count
FROM kafka_user_events
GROUP BY event_type, TUMBLE(event_time, INTERVAL '1' HOUR);
```

## ğŸ” é«˜çº§åŠŸèƒ½

### 1. æ—¶é—´æ—…è¡ŒæŸ¥è¯¢

```sql
-- æŸ¥çœ‹è¡¨å¿«ç…§
SHOW SNAPSHOTS FROM user_events;

-- æ—¶é—´æ—…è¡ŒæŸ¥è¯¢ï¼ˆæŸ¥è¯¢ç‰¹å®šæ—¶é—´ç‚¹çš„æ•°æ®ï¼‰
SELECT * FROM user_events FOR SYSTEM_TIME AS OF '2024-01-01 10:00:00';
```

### 2. å¢é‡æŸ¥è¯¢

```sql
-- å¢é‡æŸ¥è¯¢ï¼ˆæŸ¥è¯¢æŒ‡å®šå¿«ç…§èŒƒå›´çš„æ•°æ®ï¼‰
SELECT * FROM user_events FOR SYSTEM_VERSION AS OF 1;
SELECT * FROM user_events FOR SYSTEM_VERSION AS OF 2;
```

### 3. æ•°æ®æ¹–ä¼˜åŒ–

```sql
-- åˆ›å»ºä¼˜åŒ–è¡¨
CREATE TABLE user_events_optimized (
    user_id BIGINT,
    event_type STRING,
    event_time TIMESTAMP(3),
    properties STRING,
    event_date AS DATE(event_time),
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'bucket' = '8',  -- å¢åŠ bucketæ•°é‡
    'bucket-key' = 'user_id',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy',
    'target-file-size' = '64MB',  -- ä¼˜åŒ–æ–‡ä»¶å¤§å°
    'write-buffer-size' = '128MB'  -- ä¼˜åŒ–å†™å…¥ç¼“å†²åŒº
) PARTITIONED BY (event_date);
```

## ğŸ“ˆ ç›‘æ§å’Œç®¡ç†

### 1. æŸ¥çœ‹è¡¨ä¿¡æ¯

```sql
-- æŸ¥çœ‹è¡¨ç»“æ„
DESCRIBE user_events;

-- æŸ¥çœ‹è¡¨æ–‡ä»¶
SHOW FILES FROM user_events;

-- æŸ¥çœ‹è¡¨ç»Ÿè®¡ä¿¡æ¯
SHOW STATISTICS FROM user_events;
```

### 2. æ•°æ®æ¹–ç®¡ç†

```sql
-- åˆ é™¤è¿‡æœŸæ•°æ®
DELETE FROM user_events WHERE event_date < CURRENT_DATE - INTERVAL '30' DAY;

-- æ¸…ç†å¿«ç…§
-- æ³¨æ„ï¼šè¿™éœ€è¦åœ¨Flink SQL Clientä¸­æ‰§è¡Œ
-- CALL paimon.system.cleanup('user_events', '30d');
```

### 3. æ€§èƒ½ç›‘æ§

```sql
-- åˆ›å»ºç›‘æ§è¡¨
CREATE TABLE paimon_metrics (
    table_name STRING,
    metric_time TIMESTAMP(3),
    metric_type STRING,
    metric_value BIGINT,
    PRIMARY KEY (table_name, metric_time, metric_type) NOT ENFORCED
) WITH (
    'bucket' = '4',
    'bucket-key' = 'table_name',
    'changelog-producer' = 'input',
    'merge-engine' = 'deduplicate',
    'file.format' = 'parquet',
    'compression' = 'snappy'
) PARTITIONED BY (metric_time);

-- æ’å…¥ç›‘æ§æ•°æ®
INSERT INTO paimon_metrics
SELECT 
    'user_events' as table_name,
    CURRENT_TIMESTAMP as metric_time,
    'record_count' as metric_type,
    COUNT(*) as metric_value
FROM user_events
WHERE event_date = CURRENT_DATE;
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### 1. å­˜å‚¨é…ç½®

```sql
-- ä¼˜åŒ–å­˜å‚¨é…ç½®
CREATE TABLE optimized_table (
    id BIGINT,
    data STRING,
    ts TIMESTAMP(3)
) WITH (
    'bucket' = '8',                    -- å¢åŠ å¹¶è¡Œåº¦
    'bucket-key' = 'id',               -- åˆ†åŒºé”®
    'changelog-producer' = 'input',    -- å˜æ›´æ—¥å¿—ç”Ÿäº§è€…
    'merge-engine' = 'deduplicate',    -- åˆå¹¶å¼•æ“
    'file.format' = 'parquet',         -- æ–‡ä»¶æ ¼å¼
    'compression' = 'snappy',          -- å‹ç¼©æ ¼å¼
    'target-file-size' = '64MB',       -- ç›®æ ‡æ–‡ä»¶å¤§å°
    'write-buffer-size' = '128MB',     -- å†™å…¥ç¼“å†²åŒºå¤§å°
    'read.buffer.size' = '32MB',       -- è¯»å–ç¼“å†²åŒºå¤§å°
    'file.expiration' = '7d',          -- æ–‡ä»¶è¿‡æœŸæ—¶é—´
    'snapshot.expire.duration' = '7d'  -- å¿«ç…§è¿‡æœŸæ—¶é—´
) PARTITIONED BY (ts);
```

### 2. æ€§èƒ½è°ƒä¼˜

```sql
-- è®¾ç½®Flinké…ç½®
SET 'table.exec.mini-batch.enabled' = 'true';
SET 'table.exec.mini-batch.size' = '1000';
SET 'table.exec.mini-batch.allow-latency' = '1s';
SET 'table.optimizer.agg-phase-strategy' = 'TWO_PHASE';
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. è¡¨è®¾è®¡

- **åˆ†åŒºç­–ç•¥**: æŒ‰æ—¶é—´åˆ†åŒºï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½
- **Bucketæ•°é‡**: æ ¹æ®æ•°æ®é‡å’Œå¹¶è¡Œåº¦è®¾ç½®
- **ä¸»é”®è®¾è®¡**: é€‰æ‹©é«˜åŸºæ•°å­—æ®µä½œä¸ºä¸»é”®
- **æ–‡ä»¶æ ¼å¼**: ä½¿ç”¨Parquetæ ¼å¼ï¼Œæ”¯æŒåˆ—å¼å­˜å‚¨

### 2. æ•°æ®å†™å…¥

- **æ‰¹é‡å†™å…¥**: ä½¿ç”¨æ‰¹é‡æ’å…¥æé«˜æ€§èƒ½
- **æµå¼å†™å…¥**: æ”¯æŒå®æ—¶æ•°æ®æµå†™å…¥
- **äº‹åŠ¡ä¿è¯**: æ”¯æŒACIDäº‹åŠ¡

### 3. æŸ¥è¯¢ä¼˜åŒ–

- **åˆ†åŒºè£å‰ª**: åˆ©ç”¨åˆ†åŒºæé«˜æŸ¥è¯¢æ•ˆç‡
- **åˆ—è£å‰ª**: åªæŸ¥è¯¢éœ€è¦çš„åˆ—
- **è°“è¯ä¸‹æ¨**: åˆ©ç”¨ç´¢å¼•åŠ é€ŸæŸ¥è¯¢

### 4. è¿ç»´ç®¡ç†

- **å®šæœŸæ¸…ç†**: æ¸…ç†è¿‡æœŸæ•°æ®å’Œå¿«ç…§
- **ç›‘æ§å‘Šè­¦**: ç›‘æ§è¡¨å¤§å°å’ŒæŸ¥è¯¢æ€§èƒ½
- **å¤‡ä»½æ¢å¤**: å®šæœŸå¤‡ä»½é‡è¦æ•°æ®

## ğŸš¨ å¸¸è§é—®é¢˜

### 1. è¿æ¥é—®é¢˜

**é—®é¢˜**: æ— æ³•è¿æ¥åˆ°Paimon Catalog
**è§£å†³**: æ£€æŸ¥Flinké…ç½®å’ŒS3è¿æ¥

```sql
-- æ£€æŸ¥Catalogé…ç½®
SHOW CATALOGS;
USE CATALOG paimon;
```

### 2. æ€§èƒ½é—®é¢˜

**é—®é¢˜**: æŸ¥è¯¢é€Ÿåº¦æ…¢
**è§£å†³**: ä¼˜åŒ–è¡¨ç»“æ„å’ŒæŸ¥è¯¢

```sql
-- æŸ¥çœ‹è¡¨ç»Ÿè®¡ä¿¡æ¯
SHOW STATISTICS FROM table_name;

-- ä¼˜åŒ–æŸ¥è¯¢
EXPLAIN SELECT * FROM table_name WHERE partition_col = 'value';
```

### 3. å­˜å‚¨é—®é¢˜

**é—®é¢˜**: å­˜å‚¨ç©ºé—´ä¸è¶³
**è§£å†³**: æ¸…ç†è¿‡æœŸæ•°æ®å’Œå¿«ç…§

```sql
-- åˆ é™¤è¿‡æœŸæ•°æ®
DELETE FROM table_name WHERE date_col < CURRENT_DATE - INTERVAL '30' DAY;
```

## ğŸ“š æ›´å¤šèµ„æº

- [Paimonå®˜æ–¹æ–‡æ¡£](https://paimon.apache.org/docs/)
- [Flink SQLæ–‡æ¡£](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/)
- [æµå¼æ•°æ®æ¹–æœ€ä½³å®è·µ](https://paimon.apache.org/docs/master/best-practices/)

## ğŸ¤ è·å–å¸®åŠ©

- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£](../README.md)
- **ç¤ºä¾‹**: [Paimonç¤ºä¾‹](../examples/paimon-examples.sql)
- **é…ç½®**: [Paimoné…ç½®](../config/paimon/)

---

**æ³¨æ„**: æœ¬æŒ‡å—åŸºäºPaimon 0.6.0ç‰ˆæœ¬ï¼Œè¯·æ ¹æ®å®é™…ç‰ˆæœ¬è°ƒæ•´é…ç½®å‚æ•°ã€‚ 