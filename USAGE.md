# K8så¤§æ•°æ®AIå¹³å° - ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸€é”®éƒ¨ç½²

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd k8s-bigdata-ai-platform

# ä¸€é”®éƒ¨ç½²
./quick-start.sh
```

### 2. åˆ†æ­¥éƒ¨ç½²

```bash
# åˆå§‹åŒ–ç¯å¢ƒ
make init

# éƒ¨ç½²å¹³å°
make deploy

# éªŒè¯éƒ¨ç½²
make verify

# æŸ¥çœ‹è®¿é—®ä¿¡æ¯
make access-info
```

## ğŸ“‹ å¹³å°ç»„ä»¶

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | è®¿é—®åœ°å€ | é»˜è®¤è´¦å· |
|------|------|----------|----------|
| **Flink** | æµå¤„ç†å¼•æ“ | http://flink.your-domain.com | - |
| **Doris** | æ•°æ®ä»“åº“ | http://doris.your-domain.com | root/root |
| **MinIO** | å¯¹è±¡å­˜å‚¨ | http://minio.your-domain.com | admin/minio123 |
| **Kafka** | æ¶ˆæ¯é˜Ÿåˆ— | kafka:9092 | - |
| **KubeFlow** | æœºå™¨å­¦ä¹  | http://jupyter.your-domain.com | é¦–æ¬¡è®¿é—®åˆ›å»º |
| **Airflow** | å·¥ä½œæµè°ƒåº¦ | http://airflow.your-domain.com | admin/admin123 |
| **Superset** | æ•°æ®å¯è§†åŒ– | http://superset.your-domain.com | admin/admin123 |

### ç›‘æ§ç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | è®¿é—®åœ°å€ | é»˜è®¤è´¦å· |
|------|------|----------|----------|
| **Grafana** | ç›‘æ§é¢æ¿ | http://grafana.your-domain.com | admin/admin123 |
| **Prometheus** | æŒ‡æ ‡æ”¶é›† | http://prometheus.your-domain.com | - |
| **Jaeger** | é“¾è·¯è¿½è¸ª | http://jaeger.your-domain.com | - |

## ğŸ”§ å¸¸ç”¨æ“ä½œ

### æŸ¥çœ‹çŠ¶æ€

```bash
# æŸ¥çœ‹æ‰€æœ‰ç»„ä»¶çŠ¶æ€
make status

# æŸ¥çœ‹ç‰¹å®šç»„ä»¶çŠ¶æ€
kubectl get pods -n bigdata
kubectl get pods -n storage
kubectl get pods -n ai-ml
kubectl get pods -n data-pipeline
kubectl get pods -n monitoring
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹æ‰€æœ‰ç»„ä»¶æ—¥å¿—
make logs

# æŸ¥çœ‹ç‰¹å®šç»„ä»¶æ—¥å¿—
kubectl logs -f deployment/flink-jobmanager -n bigdata
kubectl logs -f deployment/doris-fe -n bigdata
kubectl logs -f deployment/minio -n storage
kubectl logs -f deployment/kafka -n bigdata
```

### æ•…éšœæ’æŸ¥

```bash
# æ‰§è¡Œå®Œæ•´æ•…éšœæ’æŸ¥
make troubleshoot

# æ£€æŸ¥ç‰¹å®šç»„ä»¶
./scripts/troubleshoot.sh --component flink
./scripts/troubleshoot.sh --component doris
./scripts/troubleshoot.sh --component minio
```

### æ‰©å®¹ç»„ä»¶

```bash
# æ‰©å®¹Flink TaskManager
kubectl scale deployment flink-taskmanager --replicas=4 -n bigdata

# æ‰©å®¹Doris BE
kubectl scale statefulset doris-be --replicas=5 -n bigdata

# æ‰©å®¹Kafka
kubectl scale statefulset kafka --replicas=5 -n bigdata
```

## ğŸ“Š æ•°æ®æ“ä½œ

### Flinkæµå¤„ç†

```bash
# æäº¤Flinkä½œä¸š
kubectl apply -f examples/flink-streaming/word-count-job.yaml

# æŸ¥çœ‹ä½œä¸šçŠ¶æ€
kubectl get flinkdeployments -n bigdata

# æŸ¥çœ‹ä½œä¸šæ—¥å¿—
kubectl logs -f deployment/word-count-job -n bigdata
```

### Dorisæ•°æ®ä»“åº“

```bash
# è¿æ¥Doris
mysql -h doris-fe.bigdata.svc.cluster.local -P 9030 -u root -p

# åˆ›å»ºæ•°æ®åº“
CREATE DATABASE bigdata_platform;

# åˆ›å»ºè¡¨
USE bigdata_platform;
CREATE TABLE user_events (
    user_id BIGINT,
    event_type VARCHAR(50),
    event_time DATETIME,
    properties JSON
) DUPLICATE KEY(user_id, event_time)
DISTRIBUTED BY HASH(user_id) BUCKETS 10;
```

### MinIOå¯¹è±¡å­˜å‚¨

```bash
# ä½¿ç”¨MinIOå®¢æˆ·ç«¯
kubectl run --rm -i --restart=Never --image=minio/mc:latest mc --namespace storage -- \
    mc alias set myminio http://minio:9000 admin minio123

# åˆ›å»ºbucket
kubectl run --rm -i --restart=Never --image=minio/mc:latest mc --namespace storage -- \
    mc mb myminio/data-lake

# ä¸Šä¼ æ–‡ä»¶
kubectl run --rm -i --restart=Never --image=minio/mc:latest mc --namespace storage -- \
    mc cp /path/to/file myminio/data-lake/
```

### Kafkaæ¶ˆæ¯é˜Ÿåˆ—

```bash
# åˆ›å»ºä¸»é¢˜
kubectl run --rm -i --restart=Never --image=bitnami/kafka:latest kafka-client --namespace bigdata -- \
    kafka-topics.sh --create --bootstrap-server kafka:9092 --topic user-events --partitions 3 --replication-factor 3

# å‘é€æ¶ˆæ¯
kubectl run --rm -i --restart=Never --image=bitnami/kafka:latest kafka-client --namespace bigdata -- \
    kafka-console-producer.sh --bootstrap-server kafka:9092 --topic user-events

# æ¶ˆè´¹æ¶ˆæ¯
kubectl run --rm -i --restart=Never --image=bitnami/kafka:latest kafka-client --namespace bigdata -- \
    kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic user-events --from-beginning
```

## ğŸ¤– AI/MLæ“ä½œ

### KubeFlowæœºå™¨å­¦ä¹ 

```bash
# è®¿é—®JupyterHub
# æµè§ˆå™¨è®¿é—®: http://jupyter.your-domain.com

# åˆ›å»ºNotebook
# åœ¨JupyterHubç•Œé¢ä¸­åˆ›å»ºæ–°çš„Notebook

# è¿è¡Œæœºå™¨å­¦ä¹ ä»»åŠ¡
# åœ¨Notebookä¸­ç¼–å†™å’Œè¿è¡ŒMLä»£ç 
```

### æ¨¡å‹è®­ç»ƒç¤ºä¾‹

```python
# åœ¨Jupyter Notebookä¸­è¿è¡Œ
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
data = pd.read_csv('s3://data-lake/dataset.csv')

# æ•°æ®é¢„å¤„ç†
X = data.drop('target', axis=1)
y = data['target']

# è®­ç»ƒæ¨¡å‹
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# è¯„ä¼°æ¨¡å‹
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
```

## ğŸ”„ å·¥ä½œæµè°ƒåº¦

### Airflow DAGç¤ºä¾‹

```python
# åœ¨Airflowä¸­åˆ›å»ºDAG
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'data_pipeline',
    default_args=default_args,
    description='æ•°æ®å¤„ç†ç®¡é“',
    schedule_interval=timedelta(hours=1),
)

def extract_data():
    # ä»Kafkaæå–æ•°æ®
    pass

def transform_data():
    # ä½¿ç”¨Flinkå¤„ç†æ•°æ®
    pass

def load_data():
    # åŠ è½½åˆ°Doris
    pass

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)

extract_task >> transform_task >> load_task
```

## ğŸ“ˆ æ•°æ®å¯è§†åŒ–

### Supersetä»ªè¡¨æ¿

```bash
# è®¿é—®Superset
# æµè§ˆå™¨è®¿é—®: http://superset.your-domain.com

# è¿æ¥æ•°æ®æº
# 1. ç‚¹å‡» "Data" -> "Databases"
# 2. ç‚¹å‡» "Add Database"
# 3. é€‰æ‹© "MySQL" è¿æ¥ç±»å‹
# 4. è¾“å…¥Dorisè¿æ¥ä¿¡æ¯:
#    Host: doris-fe.bigdata.svc.cluster.local
#    Port: 9030
#    Database: bigdata_platform
#    Username: root
#    Password: root

# åˆ›å»ºå›¾è¡¨
# 1. ç‚¹å‡» "Charts" -> "Add Chart"
# 2. é€‰æ‹©æ•°æ®æºå’Œè¡¨
# 3. é…ç½®å›¾è¡¨ç±»å‹å’Œå­—æ®µ
# 4. ä¿å­˜å›¾è¡¨

# åˆ›å»ºä»ªè¡¨æ¿
# 1. ç‚¹å‡» "Dashboards" -> "Add Dashboard"
# 2. æ·»åŠ å·²åˆ›å»ºçš„å›¾è¡¨
# 3. è°ƒæ•´å¸ƒå±€å’Œæ ·å¼
# 4. ä¿å­˜ä»ªè¡¨æ¿
```

## ğŸ” ç›‘æ§å‘Šè­¦

### Grafanaç›‘æ§

```bash
# è®¿é—®Grafana
# æµè§ˆå™¨è®¿é—®: http://grafana.your-domain.com

# æ·»åŠ æ•°æ®æº
# 1. ç‚¹å‡» "Configuration" -> "Data Sources"
# 2. æ·»åŠ  Prometheus æ•°æ®æº
# 3. URL: http://prometheus.monitoring.svc.cluster.local:9090

# åˆ›å»ºä»ªè¡¨æ¿
# 1. ç‚¹å‡» "Create" -> "Dashboard"
# 2. æ·»åŠ é¢æ¿
# 3. é€‰æ‹©æ•°æ®æºå’ŒæŒ‡æ ‡
# 4. é…ç½®æŸ¥è¯¢å’Œå¯è§†åŒ–

# è®¾ç½®å‘Šè­¦
# 1. åœ¨é¢æ¿ä¸­ç‚¹å‡» "Alert" æ ‡ç­¾
# 2. é…ç½®å‘Šè­¦è§„åˆ™
# 3. è®¾ç½®é€šçŸ¥æ¸ é“
```

### å…³é”®ç›‘æ§æŒ‡æ ‡

```yaml
# FlinkæŒ‡æ ‡
flink_jobmanager_job_uptime_seconds
flink_taskmanager_job_task_uptime_seconds
flink_taskmanager_job_task_buffers_inPoolUsage
flink_taskmanager_job_task_buffers_outPoolUsage

# DorisæŒ‡æ ‡
doris_fe_query_total
doris_fe_query_duration_seconds
doris_be_tablet_count
doris_be_data_size_bytes

# KafkaæŒ‡æ ‡
kafka_server_brokertopicmetrics_messagesin_total
kafka_server_brokertopicmetrics_bytesin_total
kafka_server_brokertopicmetrics_bytesout_total

# MinIOæŒ‡æ ‡
minio_bucket_usage_object_total
minio_bucket_usage_bytes_total
minio_s3_requests_total
```

## ğŸ› ï¸ è¿ç»´æ“ä½œ

### å¤‡ä»½å’Œæ¢å¤

```bash
# å¤‡ä»½å¹³å°æ•°æ®
make backup

# æ¢å¤å¹³å°æ•°æ®
make restore

# å¤‡ä»½ç‰¹å®šç»„ä»¶
kubectl get all -n bigdata -o yaml > bigdata-backup.yaml
kubectl get all -n storage -o yaml > storage-backup.yaml
```

### å‡çº§ç»„ä»¶

```bash
# å‡çº§æ‰€æœ‰ç»„ä»¶
make upgrade

# å‡çº§ç‰¹å®šç»„ä»¶
helm upgrade flink-operator apache/flink-kubernetes-operator -n bigdata
helm upgrade minio bitnami/minio -n storage
helm upgrade kafka bitnami/kafka -n bigdata
```

### æ¸…ç†èµ„æº

```bash
# æ¸…ç†å¹³å°
make clean

# æ¸…ç†ç‰¹å®šç»„ä»¶
kubectl delete namespace bigdata --ignore-not-found=true
kubectl delete namespace storage --ignore-not-found=true
kubectl delete namespace ai-ml --ignore-not-found=true
```

## ğŸ†˜ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **Podå¯åŠ¨å¤±è´¥**
   ```bash
   # æŸ¥çœ‹Podäº‹ä»¶
   kubectl describe pod <pod-name> -n <namespace>
   
   # æŸ¥çœ‹Podæ—¥å¿—
   kubectl logs <pod-name> -n <namespace>
   
   # æ£€æŸ¥èµ„æºé…é¢
   kubectl describe resourcequota -n <namespace>
   ```

2. **æœåŠ¡æ— æ³•è®¿é—®**
   ```bash
   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
   kubectl get services -n <namespace>
   
   # æ£€æŸ¥ç«¯ç‚¹
   kubectl get endpoints -n <namespace>
   
   # æµ‹è¯•ç½‘ç»œè¿æ¥
   kubectl run -it --rm --restart=Never --image=busybox:1.28 test -- nslookup <service-name>
   ```

3. **å­˜å‚¨é—®é¢˜**
   ```bash
   # æ£€æŸ¥PVCçŠ¶æ€
   kubectl get pvc --all-namespaces
   
   # æ£€æŸ¥PVçŠ¶æ€
   kubectl get pv
   
   # æ£€æŸ¥å­˜å‚¨ç±»
   kubectl get storageclass
   ```

### æ€§èƒ½ä¼˜åŒ–

1. **èµ„æºè°ƒä¼˜**
   ```bash
   # æŸ¥çœ‹èµ„æºä½¿ç”¨
   kubectl top nodes
   kubectl top pods --all-namespaces
   
   # è°ƒæ•´èµ„æºé™åˆ¶
   kubectl patch deployment <deployment-name> -n <namespace> -p '{"spec":{"template":{"spec":{"containers":[{"name":"<container-name>","resources":{"limits":{"memory":"4Gi","cpu":"2"},"requests":{"memory":"2Gi","cpu":"1"}}}]}}}}'
   ```

2. **ç½‘ç»œä¼˜åŒ–**
   ```bash
   # æ£€æŸ¥ç½‘ç»œç­–ç•¥
   kubectl get networkpolicies --all-namespaces
   
   # ä¼˜åŒ–ç½‘ç»œé…ç½®
   kubectl patch networkpolicy <policy-name> -n <namespace> -p '{"spec":{"egress":[{"to":[{"namespaceSelector":{"matchLabels":{"name":"storage"}}}],"ports":[{"protocol":"TCP","port":9000}]}]}}'
   ```

## ğŸ“š æ›´å¤šèµ„æº

- [è¯¦ç»†æ–‡æ¡£](./docs/)
- [é…ç½®è¯´æ˜](./config/)
- [ä½¿ç”¨ç¤ºä¾‹](./examples/)
- [æ•…éšœæ’æŸ¥](./scripts/troubleshoot.sh)
- [APIæ–‡æ¡£](https://kubernetes.io/docs/)
- [Helmæ–‡æ¡£](https://helm.sh/docs/)

## ğŸ¤ è·å–å¸®åŠ©

- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£](./docs/)
- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **è®¨è®º**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **é‚®ä»¶**: support@your-domain.com

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªç”Ÿäº§çº§åˆ«çš„å¹³å°ï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒå……åˆ†éªŒè¯åå†éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚ 